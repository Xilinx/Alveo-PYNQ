{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlapping Communication and Compute\n",
    "\n",
    "The *Intro to Compression* notebook introduced the key features for interacting with the data compression kernels. In this notebook we are going to explore how to maximize the total system performance of the system using the techniques for overlapping compute and communication first introduced in the *Kernel Optimization* suite of notebooks.\n",
    "\n",
    "Buliding on that example, here we want to implement the higher-level LZ4 *frame* compression format which encapsulates the compressed data with block and file-level headers. The full specification is [available online](https://android.googlesource.com/platform/external/lz4/+/HEAD/doc/lz4_Frame_format.md) but here we'll only touch on the parts absolutely necessary to create a file that can be decompressed by the software implementation.\n",
    "\n",
    "First we are going to import pynq and get a handle to the compression kernel we plan on using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pynq\n",
    "\n",
    "ol = pynq.Overlay('compression.xclbin')\n",
    "\n",
    "compress_kernel = ol.xilLz4Compress_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One piece of terminology that appears in the data compression examples designs is the *brick* which is used for the set of 8 blocks that are processed together. For this example we are going to stick with the 1 MB block size from the introduction and introduce a brick size of 8 blocks.\n",
    "\n",
    "Before starting to try overlapping the compute and communication we want to write the non-overlapped case, both to ensure that the algorithm is correct and to provide a benchmark for future optimization. To begin with we need to allocate the buffers as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 1024*1024\n",
    "BRICK_SIZE = 8*BLOCK_SIZE\n",
    "\n",
    "in_buffers = pynq.allocate((8,BLOCK_SIZE), 'u1', target=ol.bank0)\n",
    "out_buffers = pynq.allocate((8,BLOCK_SIZE), 'u1', target=ol.bank0)\n",
    "compressed_size = pynq.allocate((8,), 'u4', target=ol.bank0)\n",
    "uncompressed_size = pynq.allocate((8,), 'u4', target=ol.bank0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the core loop we can split the computation into three phases\n",
    "\n",
    " 1. Prepare the input buffers with uncompressed data\n",
    " 2. Call the accelerator\n",
    " 3. Retrieve the compressed data from the output buffers\n",
    " \n",
    "The `write_uncompressed` function below encapsulates all of the steps needed to prepare the input buffers as described in the previous notebook. The only change needed is some logic to account for the final brick of the file which is most likely not going to be a full brick. Otherwise the process of reshaping the input buffer and setting the size of each block in the brick is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_uncompressed(data, buffers, sizes):\n",
    "    total_size = min(len(data), BRICK_SIZE)\n",
    "    buffers.reshape((BRICK_SIZE,))[0:total_size] = data[0:total_size]\n",
    "    buffers.sync_to_device()\n",
    "    if total_size == BRICK_SIZE:\n",
    "        sizes[:] = BLOCK_SIZE\n",
    "        blocks = 8\n",
    "    else:\n",
    "        left = total_size\n",
    "        blocks = ((total_size - 1) // BLOCK_SIZE) + 1\n",
    "        sizes[0:blocks-1] = BLOCK_SIZE\n",
    "        sizes[blocks-1] = total_size % BLOCK_SIZE\n",
    "        sizes[blocks:8] = 0\n",
    "    sizes.sync_to_device()\n",
    "    return total_size, blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_compressed` reads the sizes of each compressed block, retrieves the data from the card and adds it to a stream. Again, most of this code is directly taken from the previous notebook with the capability to only process a partial brick. Another change is the addition of a *block header* to the output stream. This is simply the length of the block and is used by the decompressor to quickly find multiple block for parallel decompression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def read_compressed(stream, buffers, sizes, blocks):\n",
    "    sizes.sync_from_device()\n",
    "    for size, buffer in zip(sizes[0:blocks], buffers[0:blocks]):\n",
    "        subbuf = buffer[0:size]\n",
    "        subbuf.sync_from_device()\n",
    "        stream.write(struct.pack('<I', size))\n",
    "        stream.write(subbuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to create the non-overlapped reference implementation of the whole-file compressor. An LZ4 frame consists of the following parts\n",
    "\n",
    " * A header consisting of a magic string and some details of the compressed format\n",
    " * The compressed blocks, each with a size header\n",
    " * An empty block to mark the end of the file\n",
    " \n",
    "For this example we have a fixed block size of 1 MB which is encoded in the header and no additional checksums or data lengths - for the details of how this is encoded see the specification linked at the top of the notebook.\n",
    "\n",
    "Our `compress_hw` function works its way through the input data, splitting it into bricks and processing each brick in sequence. For ease of use we return the contents of the compressed stream as a binary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "LZ4_MAGIC = b'\\x04\\x22\\x4D\\x18'\n",
    "LZ4_HEADER = LZ4_MAGIC + b'\\x60\\x60\\x51'\n",
    "\n",
    "def compress_hw(raw_data):\n",
    "    view = memoryview(raw_data)\n",
    "    offset = 0\n",
    "    \n",
    "    stream = io.BytesIO()\n",
    "    stream.write(LZ4_HEADER)\n",
    "    \n",
    "    while offset < len(view):\n",
    "        brick_size, blocks = write_uncompressed(\n",
    "            view[offset:], in_buffers, uncompressed_size)\n",
    "        \n",
    "        compress_kernel.call(in_buffers, out_buffers,\n",
    "                             compressed_size, uncompressed_size,\n",
    "                             1024, brick_size)\n",
    "        \n",
    "        read_compressed(stream, out_buffers, compressed_size, blocks)\n",
    "        offset += brick_size\n",
    "    stream.write(b'\\x00\\x00\\x00\\x00')\n",
    "    stream.seek(0)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we'll use our *test_data.bin* consisting of an xclbin file as the input for the compressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data.bin', 'rb') as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "compressed_data = compress_hw(test_data).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we need to use the `lz4.frame` module to decompress our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lz4.frame\n",
    "uncompressed_data = lz4.frame.decompress(compressed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can make sure that the uncompressed data is the same as what we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompressed_data == test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling the Compressor\n",
    "\n",
    "Before we start optimizing our loop we need to get a baseline for the compressor. Jupyter provides a `%%timeit` magic which offers a simple way to see how long a function takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ms ± 6.48 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "compressed_data = compress_hw(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more programmatic interface we can use the `timeit` module. Using the `number` and `repeat` parameters we can match the results of the `%%timeit` magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13324313652036446"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "runtimes = timeit.repeat('compress_hw(test_data)', number=10, \n",
    "                        repeat=7, globals=globals())\n",
    "\n",
    "hw_average = sum(runtimes) / len(runtimes) / 10\n",
    "hw_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can use to find the throughput in MBps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531.031172128888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data) / hw_average / (1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Overlap\n",
    "\n",
    "To conceptualize what we are trying to achieve by overlapping the compute and communication it's worth considering what the activity diagram of our non-overlapped case looks like and how we would like to optimize it.\n",
    "\n",
    "![Non-overlapped execution](img/nonoverlapped.png)\n",
    "\n",
    "As we can see the accelerator remains idle for large periods while we transfer data to and from the device. Instead we would like something that look more like:\n",
    "\n",
    "![overlapped execution](img/overlapped.png)\n",
    "\n",
    "where the communication for neighboring iterations of the loop while the accelerator is busy. To achieve this we need to duplicate each buffer so that one is being processed by the accelerator while the other is being used for data transfer. We'll ping-pong between the two buffers until all of the data is processed. We will represent this by adding another dimension of size 2 to the front of all the buffer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_buffers = pynq.allocate((2,8,BLOCK_SIZE), 'u1', target=ol.bank0)\n",
    "out_buffers = pynq.allocate((2,8,BLOCK_SIZE), 'u1', target=ol.bank0)\n",
    "compressed_size = pynq.allocate((2,8), 'u4', target=ol.bank0)\n",
    "uncompressed_size = pynq.allocate((2,8), 'u4', target=ol.bank0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our desired execution trace we can sketch out the form our revised function should take - namely\n",
    "\n",
    " * We need to preload the first brick onto the device before starting our processing loop\n",
    " * We need to retrieve the last brick result after the end of the processing loop\n",
    " * The loop should begin by starting the accelerator and end with the call for wait\n",
    "\n",
    "With that structure in place and an `active` variable to keep track of which set of buffers are being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_overlapped(raw_data):\n",
    "    view = memoryview(raw_data)\n",
    "    offset = 0\n",
    "    active = 0\n",
    "    previous_blocks = 0\n",
    "    \n",
    "    stream = io.BytesIO()\n",
    "    stream.write(LZ4_HEADER)\n",
    "    \n",
    "    brick_size, blocks = write_uncompressed(\n",
    "            view[offset:], in_buffers[0], uncompressed_size[0])\n",
    "    \n",
    "    while True:\n",
    "        wh = compress_kernel.start(\n",
    "            in_buffers[active], out_buffers[active],\n",
    "            compressed_size[active], uncompressed_size[active],\n",
    "            1024, brick_size)\n",
    "        \n",
    "        active ^= 1\n",
    "        offset += brick_size\n",
    "        if previous_blocks != 0:\n",
    "            read_compressed(stream, out_buffers[active],\n",
    "                            compressed_size[active], previous_blocks)\n",
    "        previous_blocks = blocks\n",
    "        if offset < len(view):\n",
    "            brick_size, blocks = write_uncompressed(\n",
    "                view[offset:], in_buffers[active], uncompressed_size[active])\n",
    "        else:\n",
    "            break\n",
    "        wh.wait()\n",
    "    wh.wait()\n",
    "    active ^= 1\n",
    "    read_compressed(stream, out_buffers[active],\n",
    "                    compressed_size[active], previous_blocks)\n",
    "    stream.write(b'\\x00\\x00\\x00\\x00')\n",
    "    stream.seek(0)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the result using the same code snippet as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_data = compress_overlapped(test_data).read()\n",
    "test_data == lz4.frame.decompress(compressed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see our performance is drastically improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.3 ms ± 2.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "compress_overlapped(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with software we can see a 2x speedup with software on our test machine using settings to match the output of our Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 ms ± 118 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lz4.frame.compress(test_data, block_linked=False, store_size=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are bound by the limitations of our data-movement code. The Vitis Libraries have [examples and benchmarks](https://github.com/Xilinx/Vitis_Libraries/tree/b658aa5cd262d080048526ce931d4570cb931a36/data_compression/L3/benchmarks/lz4_p2p_compress) that use P2P memory transfers between the device and an SSD along with both compute kernels to drastically improve performance. For more details on the library consult the [repository](https://github.com/Xilinx/Vitis_Libraries/tree/master/data_compression).\n",
    "\n",
    "### Cleaning up\n",
    "\n",
    "You might want to *shutdown* this notebook at this point to ensure that all of the resources used are freed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Copyright 2020 (C) Xilinx, Inc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
